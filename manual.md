**Общее описание:**

Данный код представляет собой реализацию простого компилятора или интерпретатора для некоторого языка программирования. Он включает в себя лексический анализатор (лексер), синтаксический анализатор (парсер) и семантический анализатор.

**Структура кода:**

1. **Определение типов лексем (`LexTypes`) и их текстового представления (`lex_dec`):**
   - Содержит перечисление всех возможных типов токенов (лексем), которые могут встретиться в исходном коде программы.
   - `lex_dec` — список строковых представлений этих типов для удобного вывода и отладки.

2. **Словари ключевых слов и разделителей:**
   - `key_words_dict` — сопоставляет строковое представление ключевых слов их соответствующим типам из `LexTypes`.
   - `sep_words_dict` — аналогично для разделителей и операторов.

3. **Класс `Token`:**
   - Представляет отдельный токен (лексему) с его типом, значением и координатами в исходном тексте (номер строки и позиция в строке).

4. **Класс `State`:**
   - Используется для определения состояний конечного автомата в лексическом анализаторе (не полностью задействован в данном коде).

5. **Класс `Lexer` (лексический анализатор):**
   - Отвечает за преобразование исходного текста программы в последовательность токенов.
   - Обрабатывает исходный текст, распознавая ключевые слова, идентификаторы, числа, операторы и разделители.
   - Учитывает координаты каждого токена для последующего вывода ошибок с указанием места их возникновения.

6. **Класс `VarToken`:**
   - Используется для хранения информации о переменных, объявленных в программе (тип и имя переменной).

7. **Класс `Parser` (синтаксический и семантический анализатор):**
   - Анализирует последовательность токенов, полученных от лексера, проверяя корректность структуры программы в соответствии с синтаксическими правилами языка.
   - Выполняет семантические проверки, такие как объявление переменных перед их использованием, отсутствие повторного объявления и т.д.
   - При обнаружении ошибок выводит сообщения с указанием координат ошибок.

8. **Главная часть программы:**
   - Создает экземпляр лексера, передавая ему имя входного файла.
   - Выводит все токены для отладки.
   - Создает экземпляр парсера и запускает синтаксический анализ.
   - При успешном завершении выводит "CORRECT".

---

**Подробное объяснение по частям:**

### 1. Определение типов лексем (`LexTypes`) и их строковых представлений (`lex_dec`):

Класс `LexTypes` содержит числовые константы, представляющие различные типы токенов, которые могут быть распознаны лексическим анализатором. Каждому типу лексемы соответствует уникальное число.

Пример:

- `LEX_INTEGER = 1` — тип лексемы для ключевого слова `integer`.
- `LEX_IF = 6` — тип лексемы для ключевого слова `if`.
- `LEX_PLUS = 28` — тип лексемы для оператора `+`.

Список `lex_dec` содержит строковые представления типов лексем, соответствующие их числовым значениям в `LexTypes`. Это используется для удобного вывода типов токенов при отладке.

Пример:

- `lex_dec[1]` — `"LEX_INTEGER"`.
- `lex_dec[6]` — `"LEX_IF"`.
- `lex_dec[28]` — `"LEX_PLUS"`.

### 2. Словари ключевых слов и разделителей:

**`key_words_dict`:**

Это словарь, который сопоставляет строковые представления ключевых слов их типам из `LexTypes`.

Пример:

- `"integer": LexTypes.LEX_INTEGER` — ключевое слово `integer` соответствует типу `LEX_INTEGER`.
- `"if": LexTypes.LEX_IF` — ключевое слово `if` соответствует типу `LEX_IF`.

**`sep_words_dict`:**

Аналогичный словарь для разделителей и операторов.

Пример:

- `";": LexTypes.LEX_SEMICOLON` — символ `;` соответствует типу `LEX_SEMICOLON`.
- `"as": LexTypes.LEX_ASSIGN` — символ `as` (используется как оператор присваивания) соответствует типу `LEX_ASSIGN`.
- `"!=": LexTypes.LEX_NEQ` — символ `!=` соответствует типу `LEX_NEQ` (оператор "не равно").

### 3. Класс `Token`:

Класс `Token` представляет отдельную лексему и содержит:

- `type` — тип токена (одно из значений `LexTypes`).
- `value` — строковое значение токена (сам текст из исходного кода).
- `x_coord` — позиция токена в строке (номер символа).
- `y_coord` — номер строки, в которой находится токен.

Конструктор:

```python
def __init__(self, type_, value, x_coord=0, y_coord=0):
    self.type = type_
    self.value = value
    self.x_coord = x_coord
    self.y_coord = y_coord
```

### 4. Класс `State`:

Класс `State` предназначен для определения состояний конечного автомата в лексическом анализаторе. Хотя в данном коде состояния явно не используются, они могут быть задействованы при расширении функциональности лексера.

### 5. Класс `Lexer` (лексический анализатор):

**Основная задача лексера:**

- Преобразовать исходный текст программы в последовательность токенов.

**Поля класса:**

- `tokensVec` — список распознанных токенов.
- `x_coord`, `y_coord` — текущие координаты в исходном тексте (позиция в строке и номер строки).
- `c` — текущий символ, обрабатываемый лексером.
- `lexBuff` — буфер для накопления символов текущей лексемы.
- `cState` — текущее состояние конечного автомата (не используется явно в данном коде).
- `buffToken` — текущий токен, который формируется.

**Конструктор:**


```python
def __init__(self, file_name):
    self.tokensVec = []
    self.x_coord = 1
    self.y_coord = 1
    self.c = ''
    self.lexBuff = ''
    self.cState = State.S
    self.buffToken = None

    # Читаем весь файл целиком
    with open(file_name, 'r') as f:
        self.inputString = f.read()
    self.reading_mechanism(self.inputString)
    self.proc_tokens()
```

**Методы:**

- `reading_mechanism(self, inputString)` — основной метод, выполняющий лексический анализ входной строки `inputString`.
- `key_lex(self)` — проверяет, является ли накопленный в `lexBuff` текст ключевым словом.
- `sep_lex(self)` — проверяет, является ли накопленный текст разделителем или оператором.
- `num_lex(self)` — проверяет, является ли накопленный текст корректным числом.
- `id_lex(self)` — проверяет, является ли накопленный текст идентификатором.
- `check_e(self, buffStr)` — проверяет корректность числовых констант с использованием экспоненциальной нотации (например, `1.23e10`).
- `print_all_tokens(self)` — выводит все распознанные токены с их типами и значениями.
- `proc_tokens(self)` — обрабатывает список токенов, удаляя токены с типом `LEX_NULL`.

**Пошаговый принцип работы метода `reading_mechanism`:**

1. **Инициализация:**

   - Устанавливаются начальные значения индекса `i`, длины входной строки `length`, координат `x_coord` и `y_coord`.

2. **Основной цикл обработки символов:**

   - Цикл продолжается, пока индекс `i` меньше длины входной строки.

3. **Получение текущего символа:**

   - `c = inputString[i]` — текущий символ.

4. **Обработка переноса строк:**

   - Если текущий символ — `\n` или `\r`, увеличивается `y_coord` (номер строки), `x_coord` сбрасывается в 1, индекс `i` увеличивается.

5. **Пропуск пробелов и табуляций:**

   - Если текущий символ — пробел или табуляция, `x_coord` увеличивается, индекс `i` увеличивается.

6. **Обработка комментариев:**

   - Если текущий символ — `{`, начинается обработка комментария.
   - Цикл продолжается, пока не встретится символ `}` или не закончится входная строка.
   - При встрече переноса строки внутри комментария корректно обновляются `x_coord` и `y_coord`.
   - Если комментарий не закрыт (нет `}`), выводится ошибка и программа завершается.

7. **Обработка многосимвольных разделителей и операторов:**

   - Проходит по отсортированному (от более длинных к более коротким) списку разделителей из `sep_words_dict`.
   - Если подстрока входной строки начиная с позиции `i` совпадает с разделителем, создается токен соответствующего типа.
   - `x_coord` и `i` увеличиваются на длину разделителя.

8. **Обработка идентификаторов и ключевых слов:**

   - Если текущий символ — буква, начинается накопление символов в `lexBuff`.
   - Пока следующие символы являются буквами или цифрами, они добавляются в `lexBuff`, `x_coord` и `i` увеличиваются.
   - После накопления символов проверяется, является ли полученный текст ключевым словом (`key_lex()`).
     - Если да, создается токен соответствующего типа.
     - Если нет, токену присваивается тип `LEX_ID`.

9. **Обработка чисел:**

   - Если текущий символ — цифра или точка, начинается накопление символов числа в `lexBuff`.
   - Разрешены символы цифр, букв (для систем счисления), знаки `+`, `-`, точки и экспоненциальные обозначения `e`, `E`.
   - После накопления символов проверяется корректность числа (`num_lex()`).
     - Если число корректно, создается токен типа `LEX_NUM`.
     - Если нет, выводится ошибка и программа завершается.

10. **Обработка односимвольных разделителей:**

    - Если текущий символ присутствует в `sep_words_dict`, создается токен соответствующего типа.

11. **Обработка неизвестных символов:**

    - Если символ не распознан, выводится ошибка и программа завершается.

12. **Обновление координат:**

    - После обработки символа `x_coord` и `i` увеличиваются.

13. **Завершение:**

    - После обработки всей входной строки вызывается метод `clear_string()`, чтобы обработать оставшиеся символы в `lexBuff`.

### 6. Класс `VarToken`:

Используется для хранения информации о переменных, объявленных в программе.

- `varType` — тип переменной (например, `INTEGER`, `REAL`, `BOOLEAN`).
- `varName` — имя переменной.

### 7. Класс `Parser` (синтаксический и семантический анализатор):

**Основная задача парсера:**

- Проверить корректность последовательности токенов в соответствии с синтаксическими правилами языка.
- Выполнить семантические проверки (например, объявление переменных перед использованием).
- Вывести сообщения об ошибках с указанием координат в случае обнаружения нарушений.

**Поля класса:**

- `idx` — текущий индекс токена в списке `tokensVec`.
- `tokensVec` — список токенов, полученный от лексера.
- `tokenCount` — общее количество токенов.
- `tokensVar` — список объявленных переменных (`VarToken`).
- `singleTokenVar` — текущая переменная (`VarToken`), используемая при объявлении.
- `ifWhileCheck` — индекс токена, используемый для проверки условий в `if` и `while`.
- `ifWhileFlag` — флаг, указывающий, было ли найдено логическое выражение в условии.

**Конструктор:**

- Проверяет, что первый токен имеет тип `LEX_PROGRAM`. Если нет, выводится ошибка.
- Увеличивает `idx` для перехода к следующему токену.

**Основные методы:**

- `start_prog(self)` — начинает анализ программы.
- `start_vars(self)` — обрабатывает раздел объявлений переменных.
- `start_id(self, flagToken=True)` — обрабатывает идентификаторы.
- `start_begin(self)` — обрабатывает тело программы после ключевого слова `begin`.
- `start_V(self)` — обрабатывает выражения.
- `start_O(self)`, `start_S(self)`, `start_M(self)` — обрабатывают различные уровни приоритетов операций в выражениях.
- `num_check(self)` — проверяет корректность чисел.
- `id_check(self)` — проверяет, что идентификатор был объявлен как переменная.

**Пошаговый принцип работы:**

1. **Анализ программы (`start_prog`):**

   - Если после `program` следует `var`, начинается обработка объявлений переменных (`start_vars`).
   - После объявлений переменных ожидается ключевое слово `begin`.
   - Если структура программы нарушена, выводится ошибка.

2. **Обработка объявлений переменных (`start_vars`):**

   - Ожидается тип переменной (`integer`, `real`, `boolean`).
   - После типа ожидается идентификатор переменной (`start_id`).
   - Проверяется, не была ли переменная уже объявлена (семантическая проверка).
   - Если после идентификатора следует запятая, ожидаются дополнительные идентификаторы.
   - Процесс повторяется для следующих типов переменных.

3. **Обработка тела программы (`start_begin`):**

   - Обрабатываются различные конструкции языка:
     - Присваивание: `<идентификатор> as <выражение>;`
     - Цикл `for`: `for <идентификатор> as <выражение> to <выражение> do ... next;`
     - Условные операторы `if ... then ... else ...`
     - Циклы `while ... do ...`
     - Вложенные блоки `begin ... end;`
     - Операторы ввода `readln(...)` и вывода `writeln(...)`.

   - Для каждой конструкции проверяются необходимые ключевые слова и синтаксические элементы. При отсутствии ожидаемых элементов выводятся ошибки с указанием координат.

4. **Обработка выражений (`start_V`, `start_O`, `start_S`, `start_M`):**

   - Реализуется рекурсивный спуск по правилам грамматики для выражений.
   - Учитываются приоритеты операций:
     - `start_V` обрабатывает операции сравнения (`=`, `<>`, `<`, `>`, `<=`, `>=`).
     - `start_O` обрабатывает операции сложения и вычитания (`+`, `-`), а также логическое `or`.
     - `start_S` обрабатывает операции умножения и деления (`*`, `/`), а также логическое `and`.
     - `start_M` обрабатывает простые операнды: идентификаторы, числа, булевы значения, унарный оператор `not`, выражения в скобках.

5. **Семантические проверки:**

   - В методе `id_check` проверяется, объявлена ли переменная перед использованием.
   - В методе `num_check` проверяется корректность числовых констант.

6. **Обработка ошибок:**

   - При обнаружении синтаксических или семантических ошибок выводится соответствующее сообщение с указанием координат токена, на котором произошла ошибка.
   - Программа завершается вызовом `exit` с кодом ошибки.

### 8. Главная часть программы:

- Устанавливает имя входного файла (`err10.txt` в данном случае).
- Создает экземпляр лексического анализатора `Lexer`, передавая ему имя файла.
- Вызывает метод `print_all_tokens` для вывода всех токенов (может быть закомментировано или удалено в финальной версии).
- Создает экземпляр синтаксического анализатора `Parser`, передавая ему список токенов.
- Запускает синтаксический анализ вызовом `start_prog`.
- Если анализ успешно завершается, выводит "CORRECT".

---

**Пример работы программы:**

- **Входной файл (`err10.txt`):**

  ```plaintext
  program var
  integer x
  begin
      x as 10d;
  end
  ```

- **Лексер:**

  - Читает файл и разбивает его на токены.
  - Например, токены будут:
    - `LEX_PROGRAM` — `program`
    - `LEX_VAR` — `var`
    - `LEX_INTEGER` — `integer`
    - `LEX_ID` — `x`
    - `LEX_BEGIN` — `begin`
    - `LEX_ID` — `x`
    - `LEX_ASSIGN` — `as`
    - `LEX_NUM` — `10d`
    - `LEX_SEMICOLON` — `;`
    - `LEX_END` — `end`

- **Парсер:**

  - Проверяет, что программа начинается с `program`.
  - Обрабатывает раздел `var` и объявляет переменную `x` типа `INTEGER`.
  - Начинает обработку тела программы после `begin`.
  - Обрабатывает присваивание `x as 10d;`, проверяя, что `x` объявлен.
  - Завершается корректно, выводя "CORRECT".

---

**Заключение:**

Данный код реализует простейший компилятор или интерпретатор для учебного языка программирования. Он демонстрирует основные этапы анализа исходного кода:

1. **Лексический анализ:**

   - Разбивает исходный код на токены.
   - Распознает ключевые слова, идентификаторы, числа, операторы и разделители.
   - Учитывает позиции токенов для вывода сообщений об ошибках.

2. **Синтаксический и семантический анализ:**

   - Проверяет корректность структуры программы в соответствии с заданной грамматикой.
   - Выполняет проверки на правильность использования переменных.
   - Обрабатывает различные конструкции языка: объявления переменных, операторы присваивания, условные операторы, циклы и т.д.
   - Выводит подробные сообщения об ошибках с указанием места их возникновения.
